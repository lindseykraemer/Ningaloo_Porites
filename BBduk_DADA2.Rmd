---
title: "BBduk_DADA2_analysis"
author: "Lindsey Kraemer"
date: "2024-10-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**using reference database as seen in [KQ's Nat Comms Github](https://github.com/LaserKate/AGF18_MachineLearning/blob/main/ITS2_test_analysis_KQ_AGFJuveniles2018_Githubtidy.Rmd), and ...**

* Incorporating the specifications that I received from GWA which include:
  * STEP 1: The libraries were prepared using the SureSelectXT HS2 kit, so to remove the UMIs you will have to **trim the first 5 bases from the start of each read**.
  * STEP 2: trimmed adapters from adaptors.fasta, upon checking updated multiQC there were no remaining adapters left 
* Did not include trunLen=; used trunQ=2 to filter because quality scores looked very high throughout
* Used Arif et al. 2014 database for `AssignTaxonomy()` b/c:
    "Quigley et al. 2019 use a modified version of the GeoSymbio ITS2 database (Franklin et al. 2012), but this is now uncurated....We have used the Arif 2014 database with modified synthax to suit assignTaxonomy"
* I've incorporated the curated taxonomy from DADA2 and NBCI BLAST of NAs (Full_Tax.csv)
* Incorporating updated SEM_IDs for Porites (S2-P12, S2-P13, S2-P15, S2-P17, S4-P01, S4-P04, S4-P11, S4-P15)
* Running through analysis with all Porites IDs to see if any patterns stick out

#### Log into JCU HPC and call bash
```{bash, eval = FALSE}
#if on jcu server just type this into terminal
ssh jc824411@zodiac.hpc.jcu.edu.au
#enter password

#if elsewhere
ssh -Y -p 8822 jc824411@zodiac.hpc.jcu.edu.au
#enter password

source ~/.bashrc
echo $PATH
```

**NOTE:** 
All raw files from GenomicsWA are stored in a separate directory on HPC, OneDrive, and personal Hard drive so that there are 'as is' back ups.

* In any attempts of my workflow, contents of this raw_reads folder is copied into the appropriate directory folder `~/Ning_Por_Sym/round[]/raw_reads`
* any changes to workflow will be noted at the beginning of the Try#[] Rmd


## Gather data

1) Set working directory. Unzip the files, type:
```{bash}
mkdir /home/jc824411/Ning_Por_Sym/round7/

cp -r /home/jc824411/Ning_Por_Sym/round1/raw_reads/ /home/jc824411/Ning_Por_Sym/round7/

cd /home/jc824411/Ning_Por_Sym/round7/raw_reads
#gunzip *.gz
```

2) Create new file (samples.list) containing the names of samples (ie first part of R1 file names)
```{bash}
ls *R1*.fastq | cut -d '_' -f 1 > samples.list
cat samples.list | wc -l
#44 samples
```

3) Renaming files (simplifying the names of files to `*_R[12].fastq`):
```{bash}
for file in $(cat samples.list); do  mv ${file}_*R1*.fastq ${file}_R1.fastq; mv ${file}_*R2*.fastq ${file}_R2.fastq; done
```

4) Count number of lines (sequences) in all files (helpful for checking throughout process)
```{bash}
#devide line count by 4 because fastq files contain 4 line segments each
echo $(cat *R[12].fastq|wc -l)/4|bc
#19075730
```

## Now for some Pre-Trimming

### 1) Remove "UMIs" from beginning of reads

GWA specified that the first 5 bases of each F & R read need to be removed. Will `forcetrim` the 5 bases furthest to the left of each read.

```{bash}
#!/bin/bash
#PBS -o bbduk_output.log
#PBS -e bbduk_error.log
#PBS -j oe
#PBS -m ae
#PBS -N bbduk_job
#PBS -M lindsey.kraemer@my.jcu.edu.au
#PBS -l walltime=24:00:00
#PBS -l select=1:ncpus=5:mem=80gb

# Set the working directory
cd /home/jc824411/Ning_Por_Sym/round7/raw_reads

# Add this to all of your scripts
shopt -s expand_aliases
source /etc/profile.d/modules.sh

# Output some useful information about
# the job we are running
echo "Job identifier is $PBS_JOBID"
echo "Working directory is $PBS_O_WORKDIR"

# Load the container we want to use
# It is a good idea to always specify the version number
module load java/openjdk19
module load matlab/2023a

# Run your code to force trim the leftmost 5 bases
for file in $(cat samples.list); do
    ~/bin/bbmap/bbduk.sh \
        in1=${file}_R1.fastq \
        in2=${file}_R2.fastq \
        forcetrimleft=5 \
        out1=${file}_R1_NoUMI.fastq \
        out2=${file}_R2_NoUMI.fastq
done &>bbduk_NoUMI.log
```

Check to make sure all reads were kept. The above code should have just trimmed off first 5bp of each read.
```{bash}
#really long version but useful to view if you think that an error may have occurred while executing the above code^
less bbduk_NoUMI.log


#or SHORT & sweet version
echo $(cat *_NoUMI.fastq|wc -l)/4|bc
#19075730 yes!
```

Move all No_UMI files to their own sub-directory to keep things organised and steps separate.
```{bash}
cd /home/jc824411/Ning_Por_Sym/round7/
mkdir NoUMI
mv /home/jc824411/Ning_Por_Sym/round7/raw_reads/*_NoUMI* NoUMI 
#and copy samples.list to this directory for next step
cp /home/jc824411/Ning_Por_Sym/round7/raw_reads/samples.list NoUMI
#copy adaptors file
cp raw_reads/adaptors.fasta NoUMI/
#check sub-dir
cd /home/jc824411/Ning_Por_Sym/round7/NoUMI
ls
```

### 2) Remove reads containing Illumina sequencing adapters:

Trim adapters; use [bbduk](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbduk-guide/) 

More info on bbduk guide [here.](https://github.com/BioInfoTools/BBMap/blob/master/docs/guides/BBDukGuide.txt)

####################################
current `adaptor.fasta` describes:

>HT05

5' TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG

>TSBC12

5' GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG

######################################

Create a job, e.g., bbduk_job:
**copy all of this**
```{bash}
#!/bin/bash
#PBS -o bbduk_output.log
#PBS -e bbduk_error.log
#PBS -j oe
#PBS -m ae
#PBS -N bbduk_job
#PBS -M lindsey.kraemer@my.jcu.edu.au
#PBS -l walltime=24:00:00
#PBS -l select=1:ncpus=5:mem=80gb

# Set the working directory
cd /home/jc824411/Ning_Por_Sym/round7/NoUMI

# Add this to all of your scripts
shopt -s expand_aliases
source /etc/profile.d/modules.sh

# Output some useful information about
# the job we are running
echo "Job identifier is $PBS_JOBID"
echo "Working directory is $PBS_O_WORKDIR"

# Load the container we want to use
# It is a good idea to always specify the version number
module load java/openjdk19
module load matlab/2023a

# Run your code 
for file in $(cat samples.list); do
    ~/bin/bbmap/bbduk.sh \
        in1=${file}_R1_NoUMI.fastq \
        in2=${file}_R2_NoUMI.fastq \
        ref=adaptors.fasta \
        k=12 hdist=1 tpe tbo \
        out1=${file}_R1_NoAdapt.fastq \
        out2=${file}_R2_NoAdapt.fastq
done &>bbduk_NoAdapt.log

#This script only works in HPC system if you call bbduk from the absolute path to its source directory
#ref=<contaminant files>
#k=12 Kmer length used for finding contaminants.Contaminants shorter than k will not be found. k must be at least 1.
#tpe (trimpairsevenly) which specifies to trim both reads to the same length- (in the event that an adapter kmer was only detected in one of them)  
#tbo (trimbyoverlap) specifies to also trim adapters based on pair overlap detection using BBMerge- (which does not require known adapter sequences)
#hdist=1 Maximum Hamming distance for ref, allows one mismatch
#out[12]==<Write reads here that do not contain kmers matching the database>
```

Check to make sure most reads were kept
```{bash}
echo $(cat *_NoAdapt.fastq|wc -l)/4|bc
#18927218 when using ncpus=5:mem=80gb

# 0.8% of reads removed
```


Move all No_Adapt files to their own sub-directory to keep things organised and steps separate.
```{bash}
cd /home/jc824411/Ning_Por_Sym/round7/
mkdir NoAdapt
mv /home/jc824411/Ning_Por_Sym/round7/NoUMI/*_NoAdapt* NoAdapt 
#and copy samples.list to this directory for next step
cp /home/jc824411/Ning_Por_Sym/round7/NoUMI/samples.list NoAdapt
#check sub-dir
cd /home/jc824411/Ning_Por_Sym/round7/NoAdapt
```

### 3) Retain only PE reads that match amplicon primer.

For these samples, ITS-DINO and ITS2Rev2 ([Hume et al 2018](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5970565/)) aka ITS-Dino (for) and LO (rev) from [Pochon et al 2012](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029816) - see Table S3. 

>SymPochITS2F (ITS-Dino)

5’-GTGAATTGCAGAACTCCGTG-3’

>SymPochITS2R (ITS2Rev2)

5’-CCTCCGCTTACTTATATGCTT-3’

Code note: restrictleft set based on primer length. Also note that outputted `NoITS` files still have primer attached. 
```{bash}
#!/bin/bash
#PBS -o NoITS_output.log
#PBS -e NoITS_error.log
#PBS -j oe
#PBS -m ae
#PBS -N NoITS_job
#PBS -M lindsey.kraemer@my.jcu.edu.au
#PBS -l walltime=24:00:00
#PBS -l select=1:ncpus=8:mem=80gb

# Set the working directory
/home/jc824411/Ning_Por_Sym/round7/NoAdapt

# Add this to all of your scripts
shopt -s expand_aliases
source /etc/profile.d/modules.sh

# Output some useful information about
# the job we are running
echo "Job identifier is $PBS_JOBID"
echo "Working directory is $PBS_O_WORKDIR"

# Load the container we want to use
# It is a good idea to always specify the version number
module load java/openjdk19
module load matlab/2023a

for file in $(cat samples.list); do
    ~/bin/bbmap/bbduk.sh \
        in1=${file}_R1_NoAdapt.fastq \
        in2=${file}_R2_NoAdapt.fastq \
        restrictleft=21 \
        k=10 \
        literal=GTGAATTGCAGAACTCCGTG,CCTCCGCTTACTTATATGCTT \
        outm1=${file}_R1_NoITS.fastq \
        outm2=${file}_R2_NoITS.fastq \
        out=${file}_stats.txt \
        outu1=${file}_R1_check.fastq \
        outu2=${file}_R2_check.fastq
done &>bbduk_NoITS.log

#restrictleft=If positive, only look for kmer matches in the leftmost X bases
#outm[12]=<reads that fail filters> ie any reads with a matching kmer, literal
#outu[12]=<reads that pass all filtering criteria>
#do we need to trim primers now? NO doing this later in R DADA2 trimming/filtering 
```

Gut check- most of the reads are still there, right?!
```{bash}
echo $(cat *_NoITS.fastq|wc -l)/4|bc
# 18901656 yes!

#0.1% reads removed
```

Move all NoITS & check files to their own sub-directory to keep things organised and steps separate.
```{bash}
cd /home/jc824411/Ning_Por_Sym/round7/
mkdir NoITS check
mv /home/jc824411/Ning_Por_Sym/round7/NoAdapt/*_NoITS* NoITS 
mv /home/jc824411/Ning_Por_Sym/round7/NoAdapt/*check.fastq check/

```

#### **Added metadata and reference data and ITS2 BLAST database to NoITS folder via Mountainduck**

`mec12869-sup-0001-files1.fasta` is ITS2 BLAST database, (S1) from [Arif 2014](https://onlinelibrary.wiley.com/doi/full/10.1111/mec.12869) is the same as `arif_ITS2_DB_mod.fasta` from KQ's github. This will be used to BLAST results from assignTaxonomy() 


### 4) Copy metadata and Refernce Tax sequences over to round[] NoITS folder (for future you)
```{bash}
#move back to Nin_Por_Sym/
cd ..
cp round7/NoITS/arif_ITS2_DB_mod.fasta round[]/NoITS/
cp round7/NoITS/mec12869-sup-0001-files1.fasta round[]/NoITS/
cp round1/NoITS/meta_variables_forphyloseq.csv round7/NoITS/
#check to confirm copied
cd round7/NoITS/
ls
```


### 5) Quality Check: Inspect files using fastqc/multiqc 

Use this to check adapter content (should be 0 now across all reads), sequence length distribution, etc. 

Move a directory for zipped files to keep separate from those going into analysis 
```{bash}
cd ..
mkdir Zipped
#copy samples.list to NoITS
cp /home/jc824411/Ning_Por_Sym/round7/NoUMI/samples.list NoITS/
# copy fastq files from NoITS/ to Zipped/ so that you can do this inspection without messing with original fastq files.  
cd NoITS
for file in $(cat samples.list); do  cp ${file}_*R1*.fastq ../Zipped; cp ${file}_*R2*.fastq ../Zipped; done
```

Now run fastqc on zipped NoITS fastq files.
```{bash}
cd ../Zipped/
#zip files
gzip *.fastq
#run fastqc zipped files 
fastqc ./*.fastq.gz
```

Output will be two new files inside `Zipped`. Most interesting file is `.html` file. If you click on this file in R and select `Open in Browser` you should be able to view it.
```{bash}
#make a sub-dir for html files
mkdir ./fastqc_html
#move html files in there for easy access
mv ./*.html fastqc_html
#can view these individually
```

Run MultiQC to see info on all files in one go
```{bash}
multiqc *_fastqc.zip
#will output an html file, click this
```

# Now in R

```{bash}
#when using JCU HPC
R
```

## Call libraries

```{r}
library(dada2); packageVersion("dada2"); citation("dada2")
library(ggplot2); packageVersion("ggplot2")
library(tidyverse); packageVersion("tidyverse")
```

# Start of DADA2 Analysis
  
```{r}
  # Set path to trimmed fastq files
setwd("/home/jc824411/Ning_Por_Sym/round7/NoITS")
getwd() #to confirm

path <- "/home/jc824411/Ning_Por_Sym/round7/NoITS"
fns <- list.files(path)
fns
```

## Trimming/Filtering 

```{r}
fastqs <- fns[grepl(".fastq$", fns)] %>% sort() 
#grepl returns a logical vector 
#sort ensures forward/reverse reads are in same order

fnFs <- fastqs[grepl("_R1_", fastqs)] # Just the forward read files
fnRs <- fastqs[grepl("_R2_", fastqs)] # Just the reverse read files
```

Get sample names, assuming files named as so: SAMPLENAME_*.fastq; OTHERWISE MODIFY
```{r}
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1) #Get sample names, pulling out the first part of the sample file -- important for downstream analyses, need to be able to match to your metadata

#Specify the full path to the fnFs and fnRs
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)
```

## Visualize raw data
```{r}
#First, lets look at quality profile of R1 reads from multiple samples
  # Save plot to a PDF file to view
pdf("R1_raw_quality7.pdf")
plotQualityProfile(fnFs[c(1,2,3,4,30,31,32,33,43)])
dev.off()  # Close the graphics device 

#Can look at these pdfs in MountainDuck > ~/Ning_Por_Sym/round7/NoITS
#The quality for forward reads drops to ~30 at 250bp

#Then look at quality profile of R2 reads
pdf("R2_raw_quality7.pdf")
plotQualityProfile(fnRs[c(1,2,3,4,30,31,32,33,43)])
dev.off()

#The quality for forward reads drops below 30 at 250bp
```

* Quality above 30 is very good (kind of an arbitrary cutoff)
* Do not need to check every file- usually they should be similar

## Filtering 

```{r}
#Make directory and filenames for the filtered fastqs 
#makes a path to rename files after the filtering step
#"-d"= existence and directory
filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)

#Filter
#specify where your F and R reads and where you want to put them after filtering
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
```

```{r}
#DADA does not allow Ns (ambiguous bases)
#max amount of estimated errors, this function did better at approximating the quality of the read than just the average quality score where EE = sum(10^(-Q/10)) (2 is fine baseline, here we allow 1 expected errors- we have high quality reads, lots of read depth so we can be stringent)
#remove reads with quality score below 20
#ITS2 primers = F 20bp; R 21bp
#remove reads matching phiX genome
#enforce matching between id-line sequence identifiers of F and R reads
#compress reduces memory needed by gzipping files
#On Windows set multithread=FALSE (TRUE allows multiple FASTQ files to be processed in parallel, speeds up processing in time, does not work on Windows)
#truncLen= We're not using this as detailed below:
#"For ITS sequencing, it is usually undesirable to truncate reads to a fixed length due to the large length variation at that locus. That is OK, just leave out truncLen"- DADA2 tutorial on Github

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
              maxN=0, 
              maxEE=c(1,1), 
              truncQ=2, 
              trimLeft=c(20,21),   
              rm.phix=TRUE, 
              matchIDs=TRUE, 
              compress=TRUE, multithread=TRUE) 

head(out) 

tail(out)
```

## Learn Error Rates 

The DADA2 algorithm makes use of a parametric error model and every amplicon dataset has a different set of error rates.

```{r}
setDadaOpt(MAX_CONSIST=30) #increase number of cycles to allow convergence if necessary

errF <- learnErrors(filtFs, multithread=TRUE)

#OUTPUT:
#105039743 total bases in 464894 reads from 3 samples will be used for learning the error rates.

errR <- learnErrors(filtRs, multithread=TRUE)

#OUTPUT:
#104574849 total bases in 464894 reads from 3 samples will be used for learning the error rates.
```

"sanity check: visualize estimated error rates"

**Do black line & black dots fit closely together?**
```{r}
#error rates should decline with increasing quality score
#red line = error rates expected under nominal definition of the Q-score
#black line = estimated error rate after convergence of machine-learning algorithm
#dots = observed error rate for each quality score

pdf("plot_est_errors_Fs7.pdf")
plotErrors(errF, nominalQ=TRUE)
dev.off()

pdf("plot_est_errors_Rs7.pdf")
plotErrors(errR, nominalQ=TRUE)
dev.off()

#different nucleotides have different probabilites of being called incorrectly due to technology.Probabilities assigned to different changes based on different chemical reactions happening during sequencing
```

## Dereplicate reads 

```{r}
#"Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance”: the number of reads with that unique sequence. 
#DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads.

#need to make sure file still exists
exists <- file.exists(filtFs)
derepFs <- derepFastq(filtFs[exists], verbose=TRUE)
derepRs <- derepFastq(filtRs[exists], verbose=TRUE)

names(derepFs) <- sample.names[exists]
#can type: names(derepFs) to see if all 44 samples still exist

names(derepRs) <- sample.names[exists]
#see if all 44 reverse samples still exist
```

## Infer Sequence Variants 

```{r}
#main part of DADA2!
#Uses error model developed earlier, calculates abundance p values for each unique sequence
#tests null hypothesis that a sequence with given error rate is too abundant to be explained by sequencing errors
#Basically trying to show that statistically these ones are the most likely to be biologically meaningful differences

#"Must change some of the DADA options b/c original program optimized for 16S/18S,note ITS - from github, "We currently recommend BAND_SIZE=32 for ITS data."

setDadaOpt(BAND_SIZE=32)

dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

#"now, look at the dada class objects by sample
#will tell how many 'real' variants in unique input seqs

#inspect for various samples
dadaFs[[3]]

#OUTPUT:
#dada-class: object describing DADA2 denoising results
#49 sequence variants were inferred from 16312 input unique sequences.
#Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 32

dadaRs[[3]]

#OUTPUT:
#dada-class: object describing DADA2 denoising results
#53 sequence variants were inferred from 17666 input unique sequences.
#Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 32
```

## Merge paired reads

```{r}
#"To further cull spurious sequence variants merge the denoised forward and reverse reads, Paired reads that do not exactly overlap are removed"

mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
#Taking all data (derepFs & derepRs) in addition to info that states "these are true sequence variants" (dadaFs & dadaRs) and merging them

#Inspect the merger data.frame from samples
head(mergers[[3]])

summary((mergers[[3]]))
```

"We now have a data.frame for each sample with the merged `$sequence`, its `$abundance`,and the indices of the merged `$forward` and `$reverse` denoised sequences. Paired reads that did not exactly overlap were removed by mergePairs.

`nmatch` means it has found 151 basepairs in the middle that overlap - these essentially will be best base pairs found b/c they match

## Construct sequence table 

```{r}
#this is "A higher-resolution version of the “OTU table” produced by classical methods where we have our sequences and relative abundances in one table
#"The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. 

seqtab <- makeSequenceTable(mergers)
dim(seqtab) 
#44 samples and 4061 ASVs

#Inspect distribution of sequence lengths (aka sizes of merged amplicons)
table(nchar(getSequences(seqtab)))

#Can see that most of ASVs are 299 - 300 base pairs long. This is what was expected. 

#Good to check what you expected vs what is displayed. If you get peak that is way different than expected then a good place to pause and reflect on why (maybe primers picked up something different etc)

pdf("plot_sequence_table7.pdf")
plot(table(nchar(getSequences(seqtab))))
dev.off() 

#Most ASVs fall between 294-304
#Sequences that are much longer or shorter than expected may be the result of non-specific priming, and may be worth removing 
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(294,304)] 

table(nchar(getSequences(seqtab2)))
dim(seqtab2) 
#44 samples and 3489 ASVs
```

## Remove chimeras 

```{r}
#make sure to use seqtab2
seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
#Identified 2304 bimeras out of 3489 input sequences (66%).
#"The fraction of chimeras varies based on factors including experimental procedures and sample complexity, but can be substantial.
#BUT those variants account for only a minority of the total sequence reads
#Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though)"

dim(seqtab.nochim) # 44 1185 remain

sum(seqtab.nochim)/sum(seqtab2)
#There are 94.7% of reads left
```

## Track Read Stats 

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab2), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names

#inspect
head(track)

tail(track)

#create CSV file for track read stats
path <- "/home/jc824411/Ning_Por_Sym/round7/NoITS"
write.csv(track,file="ReadFilterStats_Ning_round7.csv",row.names=TRUE,quote=FALSE)

# Now, save outputs so can come back to the analysis stage at a later point if desired
saveRDS(seqtab.nochim, file="Seqtab_nochim_Ning_round7.rds")
```

## Preparing to Assign Taxonomy 

```{r}
#First, output fasta file for 'ASVs'
path <- "/home/jc824411/Ning_Por_Sym/round7/NoITS/seqtab.nochim.fasta"
uniquesToFasta(seqtab.nochim, path, ids = NULL, mode = "w", width = 20000)

seqtab.nochim_full <- seqtab.nochim
#then, rename output table and write it out
ids <- paste0("sq", seq(1, length(colnames(seqtab.nochim))))
colnames(seqtab.nochim)<-ids

write.csv(seqtab.nochim_full,file="Full_AllASVs.csv",quote=F) #this is your full ASV counts table

write.csv(seqtab.nochim,file="OutputDADA_AllASVs.csv",quote=F) #this is your easily read-able ASV counts table

#can check the structure 
str(seqtab.nochim)
```

R in HPC should prompt you to save workspace everytime you q() but just in case:
```{r}
save.image("round7_workspace.RData")
#to reopen
load("round7_workspace.RData")
```

## Assigning Taxonomy

DADA2 provides a native implementation of the **RDP's naive Bayesian classifier**. The assignTaxonomy function takes a set of sequences and a training set of taxonomically classified sequences, and outputs the taxonomic assignments with at least minBoot [bootstrap confidence](https://acclab.github.io/bootstrap-confidence-intervals.html).

The reference sequence with the most similar profile is used to assign taxonomy to the query sequence, and then a bootstrapping approach is used to assess the confidence assignment at each taxonomic level. See [this](https://journals.asm.org/doi/full/10.1128/aem.00062-07) for more information on bootstrapping and naive Bayesian classifier.

Here, I am using the Arif et al. 2014 database with modified synthax to suit assignTaxonomy 

```{r}
#make sure you use the seqtab.nochim_full, otherwise you'll get an error because you will not be using actual unique sequences

#For taxLevels = the reference database only has one level so clarifying that the level = "ITS2_types"

#lowered bootstrap to 5 because was not getting many hits with default

taxa5 <- assignTaxonomy(seqtab.nochim_full, "/home/jc824411/Ning_Por_Sym/round7/NoITS/arif_ITS2_DB_mod.fasta", minBoot=5,multithread=TRUE,tryRC=TRUE,outputBootstraps=FALSE, taxLevels = "ITS2_type")
unname(head(taxa5, 30))
unname(taxa5)
```

For now save RDS so you can come back to the analysis stage at a later point if desired
```{r}
saveRDS(seqtab.nochim_full, file="round7_seqtab_nochim_full.rds")
saveRDS(taxa5, file="round7_taxa_blastCorrected.rds")

#If you need to read in previously saved datafiles
ASV_full <- readRDS("round7_seqtab_nochim_full.rds")
taxa <- readRDS("round7_taxa_blastCorrected.rds")
```

## See if assigned identities largely match separate blastn search against the same database

(A note if this is ever needed: Sybiodiniaceae NCBI taxid 252141)

Using seqtab.nochim - run blast against Symbio database to compare
```{bash}
cd $HOME/blast
#produce database 
makeblastdb -in /home/jc824411/Ning_Por_Sym/round7/NoITS/arif_ITS2_DB_mod.fasta -dbtype nucl
#blast ASVS against database
blastn -query /home/jc824411/Ning_Por_Sym/round7/NoITS/seqtab.nochim.fasta -db /home/jc824411/Ning_Por_Sym/round7/NoITS/arif_ITS2_DB_mod.fasta -num_descriptions 5 -num_alignments 5 -out NODES_All_7.br
grep -A 12 'Query=' NODES_All_7.br

#output should be in Brotli format
```

## Use LULU to collapse intragenomic variants if present 

#### In JCU HPC/ Unix command-line
```{bash}
#First produce a blastdatabase with the inferred ASVs
makeblastdb -in /home/jc824411/Ning_Por_Sym/round7/NoITS/seqtab.nochim.fasta -parse_seqids -dbtype nucl

# Then blast the inferred ASVs against the database to produce the match list which provides information about sequence similarity for collapsing ITS2 diversity 
blastn -db /home/jc824411/Ning_Por_Sym/round7/NoITS/seqtab.nochim.fasta -outfmt '6 qseqid sseqid pident' -out match_list.txt -qcov_hsp_perc 90 -perc_identity 84 -query /home/jc824411/Ning_Por_Sym/round7/NoITS/seqtab.nochim.fasta
```

#### Back in R
Doing this in R on local computer because JCU HPC doesn't have 'lulu'

```{r}
#first, read in ASV table. Trying both fullASVs and short version ("sq1") to see if there's a difference

#full
fulldat <-read.csv("Full_AllASVs.csv")
head(fulldat)
#short
alldat<-read.csv("OutputDADA_AllASVs.csv")
head(alldat)

#And match list
matchList<-read.table("match_list.txt", header=FALSE,as.is=TRUE, stringsAsFactors=FALSE)
head(matchList)

#Reformat ASV table to desired LULU format
#full
rownames(fulldat)<-fulldat$X
fASVs<-data.frame(t(fulldat[,2:1186])) 
head(fASVs)
#short
rownames(alldat)<-alldat$X
ASVs<-data.frame(t(alldat[,2:1186])) 
head(ASVs)
```

### Now, run the LULU curation [lulu info here](https://github.com/tobiasgf/lulu)
```{r}
#library(devtools)
#install_github("tobiasgf/lulu")  
library("lulu"); packageVersion("lulu") #0.1.0

full_curated_result <- lulu(fASVs, matchList) #default threshold = 95% co-occurence and 84% sequence similarity

full_curated_result_70 <- lulu(fASVs, matchList,minimum_relative_cooccurence=0.7) #using 70% co-occurrence across samples and 84% sequence similarity, for conservative comparison


full_curated_result$curated_count
full_curated_result$discarded_count


full_curated_result_70$curated_count
full_curated_result_70$discarded_count

##with short ASVs to see if there's a difference
curated_result <- lulu(ASVs, matchList)

curated_result_70 <- lulu(ASVs, matchList,minimum_relative_cooccurence=0.7) 

curated_result$curated_table
curated_result$curated_count
curated_result$discarded_count

curated_result_70$curated_table
curated_result_70$curated_count
curated_result_70$discarded_count


summary(full_curated_result)#does not discard any ASVs!

summary(curated_result)#does not discard any ASVs! 

#nothing to collapse. Proceed with full dataset. 

#Also making note that there was no difference between using Full_ASVs.csv and abbreviated version (OutputDADA_AllASVs.csv)
```

## Classifying NAs

### Blast these NAs against NCBI database to make sure they largely match Symbiodiniaceae and not another ITS2 profile (ie bacteria)
```{r}
taxa1 <- readRDS("round7_taxa_blastCorrected.rds") %>%
  as.data.frame() %>%
  mutate(ITS2_type = str_replace(ITS2_type, "^GS_", "")) %>%
  as.matrix()

taxa_csv <- as.data.frame(taxa1) %>%
  rownames_to_column() 

taxa_csv <- taxa_csv[is.na(taxa_csv$ITS2_type), ] 

asv_identifiers <- rownames(taxa_csv)
asv_sequences <- taxa_csv$rowname

# Write ASV sequences to a FASTA file
writeLines(paste(">", asv_identifiers, "\n", asv_sequences), "ASV_sequences.fasta")

write_csv(taxa_csv, file = "NA_ASV_taxonomy.csv")
```

NAs blast match Cladocopium taxa sequences on NCBI ~ +98%
Created a new csv file with full ITS2 taxonomy, no more NAs ("Full_Tax.csv")
